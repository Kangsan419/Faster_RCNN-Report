{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object Detection Through Faster RCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1U17m_6cNZnHWjQEydnSiVDww3KZIBtM8",
      "authorship_tag": "ABX9TyNwWwhZyI+oqXrggnwdGVvw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f251132a38cc4b8780ffa51fd92297f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b40c85030f1443cfbd6d69b0102c71f2",
              "IPY_MODEL_bcd55d44b85e44f284817eb0e420e2ac",
              "IPY_MODEL_fdb03d088b4b45b09e5dc851ad544311"
            ],
            "layout": "IPY_MODEL_c286e86039354e4e8391833c12712072"
          }
        },
        "b40c85030f1443cfbd6d69b0102c71f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a69721586f7e4a9b9d1ff90dfa39d814",
            "placeholder": "​",
            "style": "IPY_MODEL_b51af0d1521d4dc791c39a0921119aea",
            "value": "100%"
          }
        },
        "bcd55d44b85e44f284817eb0e420e2ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3abac43c6e374b04a5d9a22e34454f29",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ec9479905ed4f56bb4374afaff16cbf",
            "value": 553433881
          }
        },
        "fdb03d088b4b45b09e5dc851ad544311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e00eccecba644ec861f4d3319584144",
            "placeholder": "​",
            "style": "IPY_MODEL_b5260b110087412581034060352864a9",
            "value": " 528M/528M [00:05&lt;00:00, 34.9MB/s]"
          }
        },
        "c286e86039354e4e8391833c12712072": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a69721586f7e4a9b9d1ff90dfa39d814": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b51af0d1521d4dc791c39a0921119aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3abac43c6e374b04a5d9a22e34454f29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ec9479905ed4f56bb4374afaff16cbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e00eccecba644ec861f4d3319584144": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5260b110087412581034060352864a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kangsan419/Faster_RCNN-Report/blob/main/Object_Detection_Through_Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Object Detection With Faster-RCNN"
      ],
      "metadata": {
        "id": "DNu38N0zCZgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out team in Capstone Design decided to use Yolo algorithm over CNN derived algorithms due to the speed. Since we thought that real time detection and accuracy is essetial to our problem Yolo algorithms seemed to be better than CNN algorithms. \n",
        "\n",
        "However, as CNN deriven algorithms made a processes, they came up with faster processing speed with almost equal amount of accuracy. Thus I thought studying and making model out of latest CNN deriven algorithm, Faster-RCNN would help my team`s project when we have to compare and constrast each algorithms. \n"
      ],
      "metadata": {
        "id": "p42_mC6ymKzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Object Detection Through Faster RCNN.ipynb\n"
      ],
      "metadata": {
        "id": "rYwSK27dmqQL",
        "outputId": "8ca70018-aee6-4147-d137-9f49ecebcf41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 20] Not a directory: '/content/drive/MyDrive/Object Detection Through Faster RCNN.ipynb'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"rkdtks419@g.skku.edu\"\n",
        "!git config --global user.name \"Kangsan419\""
      ],
      "metadata": {
        "id": "JPDTp7i_J7mN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDJVpMAnKFAV",
        "outputId": "cd3dac44-d802-4ac5-dada-af27c4eae952"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZXRVJfQl_jl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1888fa-b211-4b7b-d41e-6a42c649eb7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Set up google drive in google colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip training data from drive\n",
        "\n",
        "%cd /content/drive/MyDrive/VOC\n",
        "\n",
        "!unzip -q '/content/drive/MyDrive/VOC/VOCdevkit.zip'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_WeHYRbwzFl",
        "outputId": "33700d37-8471-4704-bb76-3ba4785b37d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If your file is ready move directory to VOCdevkit\n",
        "%cd /content/drive/MyDrive/VOC/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcf3BtXQNXtS",
        "outputId": "614c4c76-7b3f-4459-e6e6-fc6dad8a26b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "k0pNUt6W3mQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "import math\n",
        "import xml.etree.ElementTree as ET\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image,ImageFilter, ImageEnhance\n",
        "import cv2\n",
        "import torch.optim as optim\n",
        "from torchvision.ops import nms\n",
        "from tqdm import tqdm\n",
        "import time"
      ],
      "metadata": {
        "id": "M8MQc3SOwzvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Hyperparameter"
      ],
      "metadata": {
        "id": "l7m20tjd3wbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "select_classes = {'aeroplane', 'bicycle','boat','bus', 'dog','train','motorbike'} # Only training for these classes\n",
        "\n",
        "\n",
        "\n",
        "anchor_scale = torch.FloatTensor([8,16,32])  \n",
        "anchor_ratio = torch.FloatTensor([0.5,1,2])  \n",
        "conversion_scale = 16\n",
        "input_image_height = 800\n",
        "input_image_width = 800\n",
        "num_anchors_sample = 256\n",
        "\n",
        "# Non Maximum Suppression hyper parameters\n",
        "nms_threshold = 0.7       # only consider anchors with IOU < nms_threshold\n",
        "nms_num_train_pre = 12000 # Select top nms_num_train_pre anchors to apply nms on \n",
        "nms_num_train_post = 2000 # number of proposal regions after NMS\n",
        "nms_num_test_pre = 6000   # For test \n",
        "nms_num_test_post = 300   # For test\n",
        "nms_min_size = 16         # only consider anchors as valid while applying nms if ht and wt < nms_min_size\n",
        "\n",
        "\n",
        "#proposal target (pt) hyper parameters\n",
        "pt_n_sample = 128  #Number of samples to sample from roi\n",
        "pt_pos_ratio = 0.25 #the number of positive examples out of the n_samples\n",
        "pt_pos_iou_threshold  = 0.5 #Min overlap required between roi and gt for considering positive label (object)\n",
        "pt_neg_iou_threshold = 0.5 # below this value marks as negative\n",
        "\n",
        "\n",
        "#ROI pooling\n",
        "roi_size = (7,7)\n",
        "\n",
        "# RPN loss \n",
        "rpn_loss_lambda = 1 # same is used in the decider loss\n",
        "\n",
        "\n",
        "# Number of validation images\n",
        "num_valid_img = 150\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ1uW8Nn3lya",
        "outputId": "0683ff71-a477-42bb-fe97-ee29b6b836f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle Training Data"
      ],
      "metadata": {
        "id": "23VwI9qy3-EO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get all classes and create label encoding"
      ],
      "metadata": {
        "id": "bMaELdiZHYn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting all the classes and creating label encoding\n",
        "\n",
        "all_labels = []\n",
        "\n",
        "\n",
        "for out in sorted(os.listdir('VOCdevkit/VOC2007/Annotations/')):\n",
        "    tree = ET.parse('VOCdevkit/VOC2007/Annotations/' + out)\n",
        "    for obj in tree.findall('object'):\n",
        "        lab = (obj.find('name').text)\n",
        "        # all_labels.append(lab)\n",
        "        if (lab in (select_classes)):\n",
        "          all_labels.append(lab)\n",
        "        \n",
        "distict_labels = list(set(all_labels))\n",
        "distict_labels = sorted(distict_labels)\n",
        "\n",
        "#label 0 is set for background\n",
        "lab_to_val = {j:i+1 for i,j in enumerate(distict_labels)}\n",
        "val_to_lab = {i+1:j for i,j in enumerate(distict_labels)}\n",
        "\n",
        "num_classes = len(distict_labels) + 1\n",
        "\n",
        "\n",
        "print(np.unique(np.array(all_labels), return_counts=True)[0])\n",
        "print(np.unique(np.array(all_labels), return_counts=True)[1])\n",
        "print(num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BblXQCf39kX",
        "outputId": "f0dbed38-cfc2-49c9-e841-e107e9946f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aeroplane' 'bicycle' 'boat' 'bus' 'dog' 'motorbike' 'train']\n",
            "[331 418 398 272 538 390 328]\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation"
      ],
      "metadata": {
        "id": "V2dAn-vg4ETr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random blur on training image\n",
        "def random_blur(img):\n",
        "  if random.random() < 0.5:\n",
        "    return img\n",
        "  \n",
        "  rad = random.choice([1,2])\n",
        "  img = img.filter(ImageFilter.BoxBlur(radius=rad))\n",
        "  return img\n",
        "\n",
        "\n",
        "# Random brightness, contrast, satutration and hue\n",
        "def random_color(img):\n",
        "  if random.random() < 0.1:\n",
        "    return img\n",
        "\n",
        "  img = transforms.ColorJitter(brightness=(0.5,2.0), contrast=(0.5,2.0), saturation=(0.5,2.0), hue=(-0.25,0.25))(img)\n",
        "  return img\n",
        "\n",
        "# Random horizontal flip\n",
        "def random_flip(img, gt_box):\n",
        "  if random.random() < 0.5:\n",
        "    return img,gt_box\n",
        "\n",
        "  img = transforms.RandomHorizontalFlip(p=1)(img)\n",
        "  temp = (gt_box[:,1]).copy()\n",
        "  gt_box[:,1] = img.size[0] - gt_box[:,0] #x1\n",
        "  gt_box[:,0] = img.size[0] - temp #x2\n",
        "\n",
        "  return img, gt_box\n",
        "\n",
        "\n",
        "# Random crop on image\n",
        "def random_crop(img, gt_box, labels):\n",
        "  if random.random() < 0.5:\n",
        "    return img,gt_box,labels\n",
        "  width, height = img.size\n",
        "  select_w = random.uniform(0.6*width, width)\n",
        "  select_h = random.uniform(0.6*height, height)\n",
        "\n",
        "  start_x = random.uniform(0,width - select_w)\n",
        "  start_y = random.uniform(0,height - select_h)\n",
        "\n",
        "  left = start_x\n",
        "  upper = start_y\n",
        "  right = start_x + select_w\n",
        "  bottom = start_y + select_h\n",
        "\n",
        "  gt_box_copy = gt_box.copy()\n",
        "\n",
        "  gt_box_copy[gt_box_copy[:,0] < left, 0] = left\n",
        "  gt_box_copy[gt_box_copy[:,1] > right, 1] = right\n",
        "  gt_box_copy[gt_box_copy[:,2] < upper, 2] = upper\n",
        "  gt_box_copy[gt_box_copy[:,3] > bottom, 3] = bottom\n",
        "\n",
        "  final_gt_box = []\n",
        "  final_labels = []\n",
        "\n",
        "  for i in range((gt_box_copy.shape[0])):\n",
        "    if (((gt_box_copy[i,1] - gt_box_copy[i,0])/(gt_box[i,1]-gt_box[i,0])) < 0.5):\n",
        "      continue\n",
        "    if (((gt_box_copy[i,3] - gt_box_copy[i,2])/(gt_box[i,3] - gt_box[i,2])) < 0.5):\n",
        "      continue\n",
        "    final_gt_box.append(gt_box_copy[i])\n",
        "    final_labels.append(labels[i])\n",
        "\n",
        "  if len(final_gt_box) == 0:\n",
        "    return img,gt_box,labels\n",
        "\n",
        "  final_gt_box = np.array(final_gt_box)\n",
        "  final_gt_box[:,0] = final_gt_box[:,0] - left\n",
        "  final_gt_box[:,1] = final_gt_box[:,1] - left\n",
        "  final_gt_box[:,2] = final_gt_box[:,2] - upper\n",
        "  final_gt_box[:,3] = final_gt_box[:,3] - upper\n",
        "\n",
        "  return img.crop((left, upper, right, bottom)), final_gt_box, final_labels"
      ],
      "metadata": {
        "id": "qkh-veZZ4G7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Pytorch Dataset and Dataloader"
      ],
      "metadata": {
        "id": "PV9dd0Sa4e8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pascal_voc_data(Dataset):\n",
        "    def __init__(self, img_dir,desc_dir,type_list, isTrain, transform = None):\n",
        "        super().__init__()\n",
        "        self.img_dir = img_dir\n",
        "        self.desc_dir = desc_dir\n",
        "        self.type_list = type_list\n",
        "        self.isTrain = isTrain\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "        self.img_names = []\n",
        "        self.img_descs = []\n",
        "        for img in sorted(os.listdir(img_dir)):\n",
        "          if img[:-4] in self.type_list:\n",
        "            self.img_names.append(img)\n",
        "        \n",
        "        for desc in sorted(os.listdir(desc_dir)):\n",
        "          if desc[:-4] in  self.type_list:\n",
        "            self.img_descs.append(desc)\n",
        "       \n",
        "        self.img_names = [os.path.join(img_dir, img_name) for img_name in self.img_names]\n",
        "        self.img_descs = [os.path.join(desc_dir, img_desc) for img_desc in self.img_descs]\n",
        "\n",
        "        \n",
        "        # self.img_names = sorted(os.listdir(img_dir))\n",
        "        # self.img_descs = sorted(os.listdir(desc_dir))\n",
        "        \n",
        "        # self.img_names = [os.path.join(img_dir, img_name) for img_name in self.img_names]\n",
        "        # self.img_descs = [os.path.join(desc_dir, img_desc) for img_desc in self.img_descs]\n",
        "                \n",
        "        \n",
        "        self.loc_gts = []\n",
        "        self.loc_labels = []\n",
        "        self.final_img_names = []\n",
        "        for img_idx,img_desc in enumerate(self.img_descs):\n",
        "            tree = ET.parse(img_desc)\n",
        "            gt = []\n",
        "            loc_lab = []\n",
        "            for obj in tree.findall('object'):\n",
        "              if ((obj.find('name').text) not in (select_classes)):\n",
        "                continue\n",
        "              lab = lab_to_val[(obj.find('name').text)]\n",
        "              \n",
        "              loc1 = int(obj.find('bndbox').find('xmin').text)\n",
        "              loc2 = int(obj.find('bndbox').find('xmax').text)\n",
        "              loc3 = int(obj.find('bndbox').find('ymin').text)\n",
        "              loc4 = int(obj.find('bndbox').find('ymax').text)\n",
        "\n",
        "              # if ht or width is less than 10, ignore the gt box\n",
        "              if ((loc2 - loc1) < 10 ) or ((loc4 - loc3) < 10):\n",
        "                continue\n",
        "\n",
        "              gt.append([int(loc1),int(loc2),int(loc3),int(loc4)])\n",
        "              loc_lab.append(lab)\n",
        "            if (len(gt) == 0):\n",
        "              continue\n",
        "            self.loc_gts.append(gt)\n",
        "            self.loc_labels.append(loc_lab)\n",
        "            self.final_img_names.append(self.img_names[img_idx])\n",
        "\n",
        "        self.img_names = self.final_img_names\n",
        "             \n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        img_name = self.img_names[idx]\n",
        "        img = Image.open(img_name)\n",
        "\n",
        "        arr_loc_gts = np.array(self.loc_gts[idx])\n",
        "        label = self.loc_labels[idx]\n",
        "\n",
        "        if self.isTrain:\n",
        "          img = random_blur(img)\n",
        "          img = random_color(img)\n",
        "          img,arr_loc_gts = random_flip(img,arr_loc_gts)\n",
        "          img,arr_loc_gts,label = random_crop(img,arr_loc_gts,label)\n",
        "        \n",
        "        img_h_pre = img.size[1]\n",
        "        img_w_pre = img.size[0]\n",
        "        \n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "            \n",
        "        img_h_post = img.shape[1]\n",
        "        img_w_post = img.shape[2]\n",
        "        \n",
        "        height_ratio = img_h_post/img_h_pre\n",
        "        width_ratio = img_w_post/img_w_pre\n",
        "      \n",
        "        \n",
        "        \n",
        "        arr_loc_gts[:,0] = arr_loc_gts[:,0]*width_ratio\n",
        "        arr_loc_gts[:,1] = arr_loc_gts[:,1]*width_ratio\n",
        "        arr_loc_gts[:,2] = arr_loc_gts[:,2]*height_ratio\n",
        "        arr_loc_gts[:,3] = arr_loc_gts[:,3]*height_ratio\n",
        "                        \n",
        "        gts = (arr_loc_gts).tolist()\n",
        "        \n",
        "        \n",
        "        return img, gts,label"
      ],
      "metadata": {
        "id": "u6TN3_ni4frL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "While using pretrained models - \n",
        "Pytorch torchvision documentation - https://pytorch.org/docs/master/torchvision/models.html\n",
        "The images have to be loaded in to a range of [0, 1] and then \n",
        "normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
        "'''\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((input_image_height,input_image_width)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "   std=[1/0.229, 1/0.224, 1/0.225]\n",
        ")"
      ],
      "metadata": {
        "id": "j46b6Esb5POX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into train and validations"
      ],
      "metadata": {
        "id": "QkjGfQAy5Q-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/drive/MyDrive/VOC/VOCdevkit/VOC2007/ImageSets/Segmentation/val.txt', \"r\")\n",
        "valid_images = file.read().split('\\n')\n",
        "valid_images = valid_images[:-1]\n",
        "\n",
        "\n",
        "train_images = []\n",
        "for img in os.listdir('VOCdevkit/VOC2007/JPEGImages/'):\n",
        "  if img[:-4] in valid_images:\n",
        "    continue\n",
        "  train_images.append(img[:-4])"
      ],
      "metadata": {
        "id": "RevQ7RJA5S_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = pascal_voc_data('VOCdevkit/VOC2007/JPEGImages/', 'VOCdevkit/VOC2007/Annotations/',train_images , True,transform)\n",
        "valid_dataset = pascal_voc_data('VOCdevkit/VOC2007/JPEGImages/', 'VOCdevkit/VOC2007/Annotations/',valid_images ,False,transform)"
      ],
      "metadata": {
        "id": "dZ_M2dS35VIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))\n",
        "print(len(valid_dataset))"
      ],
      "metadata": {
        "id": "7r2EB59u5Xp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "657a4bb5-c129-4201-d362-6c1225fbaa98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1690\n",
            "87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Validation class images"
      ],
      "metadata": {
        "id": "FB1M3IAr5ZgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(valid_dataset, batch_size = len(valid_dataset))\n",
        "a,b,c = iter(test_loader).next()\n",
        "print(torch.unique(c[0], return_counts=True))"
      ],
      "metadata": {
        "id": "DNKIalJH5be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "52205601-c49b-41c1-ce69-14cc8359072e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-497de6bfd3cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=1)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=1,shuffle=True)"
      ],
      "metadata": {
        "id": "luAG85jK5duV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Visualise input image with box co-ordinates provided in format - x0,x1,y0,y1"
      ],
      "metadata": {
        "id": "kgTTYnZO5hFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given input image, draw rectangles as specified by gt_box and pred_box and display\n",
        "def visualize_tensor(img, gt_box, pred_box,save_image='',tb_writer=None):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    transform_img = inv_normalize(img[0]).permute(1,2,0).to('cpu').numpy()\n",
        "    transform_img = transform_img.copy()\n",
        "    for box in gt_box:\n",
        "        x0, x1, y0, y1 = box\n",
        "        cv2.rectangle(transform_img, (int(x0),int(y0)), (int(x1),int(y1)), color=(0, 255, 255), thickness=2)\n",
        "    for box in pred_box:\n",
        "        x0, x1, y0, y1 = box\n",
        "        cv2.rectangle(transform_img, (int(x0), int(y0)), (int(x1), int(y1)), color=(255, 0, 0), thickness=2)\n",
        "    \n",
        "    if tb_writer:\n",
        "      # grid = torchvision.utils.make_grid(transform_img)\n",
        "      tb_writer.add_image(save_image, transform_img, dataformats='HWC')\n",
        "    elif save_image == '':\n",
        "        plt.imshow(transform_img)\n",
        "        plt.show()  \n",
        "    else:\n",
        "        plt.imshow(transform_img)\n",
        "        plt.savefig(save_image + '.png')"
      ],
      "metadata": {
        "id": "HqZDUKQW5gCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Faster RCNN backbone used - VGG16 model\n"
      ],
      "metadata": {
        "id": "UbeYPFOj5mNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Top 10 layers (top 4 conv layers) are not trained"
      ],
      "metadata": {
        "id": "YJg3e6NRJY1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base VGG16 model\n",
        "\n",
        "def Faster_RCNN_vgg16(num_freeze_top): \n",
        "    vgg16 = models.vgg16(pretrained=True)\n",
        "    vgg_feature_extracter  = vgg16.features[:-1]\n",
        "    vgg_classifier = vgg16.classifier[:-1]\n",
        "    \n",
        "    # Freeze learning of top few conv layers\n",
        "    for layer in vgg_feature_extracter[:num_freeze_top]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    return vgg_feature_extracter.to(device), vgg_classifier.to(device)"
      ],
      "metadata": {
        "id": "N-0luLAw5l36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code for anchor generation on an image\n"
      ],
      "metadata": {
        "id": "HUrY2eVa5tPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For train and test, only anchors lying inside the image boundary are considered"
      ],
      "metadata": {
        "id": "VSQD9UOTJfY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "    RCNN Paper\n",
        "    For anchors, we use 3 scales with box areas of (128*128) || (256*256) , and (512*512) pixels, \n",
        "    and 3 aspect ratios of 1:1, 1:2, and 2:1.\n",
        "\n",
        "'''\n",
        "\n",
        "# At point x,y from feature map, return 9 anchors on the input image scale\n",
        "def generate_anchor_at_point(x,y):\n",
        "    anchor_positions = torch.zeros((len(anchor_scale) * len(anchor_ratio),4))\n",
        "    ctr_x = (x*2+1)*(conversion_scale/2)   # for x = 0, centre is 8 || for x = 1, centre is 24\n",
        "    ctr_y = (y*2+1)*(conversion_scale/2)\n",
        "    for ratio_idx in range(len(anchor_ratio)):\n",
        "        for scale_idx in range(len(anchor_scale)):\n",
        "\n",
        "            current = len(anchor_scale)*ratio_idx + scale_idx\n",
        "            ratio = anchor_ratio[ratio_idx]\n",
        "            scale = anchor_scale[scale_idx]\n",
        "\n",
        "            h = conversion_scale*scale*torch.sqrt(ratio)\n",
        "            w = conversion_scale*scale*torch.sqrt(1.0/ratio)\n",
        "            \n",
        "            anchor_positions[current,0] = ctr_x - w/2\n",
        "            anchor_positions[current,1] = ctr_x + w/2\n",
        "            anchor_positions[current,2] = ctr_y - h/2\n",
        "            anchor_positions[current,3] = ctr_y + h/2\n",
        "            \n",
        "    return anchor_positions\n",
        "\n",
        "\n",
        "# For features of scale (x,y) , generate all the anchor boxes\n",
        "# input is height,width\n",
        "# returns output on  x*y*9,4\n",
        "def generate_anchors(x,y):\n",
        "    anchor_positions = torch.zeros((x*y,len(anchor_scale) * len(anchor_ratio),4))\n",
        "    for ctr_x in range(x):\n",
        "        for ctr_y in range(y):\n",
        "            current = ctr_x*y + ctr_y\n",
        "            anchors = generate_anchor_at_point(ctr_x, ctr_y)\n",
        "            anchor_positions[current] = anchors\n",
        "    return anchor_positions.reshape(-1,4)"
      ],
      "metadata": {
        "id": "tBDA0ULc5ruy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "RCNN paper\n",
        "    - During training, we ignore all cross-boundary anchors so they do not contribute to the loss\n",
        "    - During testing, however, we still apply the fully convolutional RPN to the entire image. This may \n",
        "    generate crossboundary proposal boxes, which we clip to the image boundary\n",
        "'''\n",
        "\n",
        "'''\n",
        "    For this code, cross boundary anchors are ignored at this step both in train and test\n",
        "'''\n",
        "\n",
        "def get_valid_anchors(anchor_positions):\n",
        "    valid_anchors_idx = torch.where((anchor_positions[:,0] >= 0) & \n",
        "             (anchor_positions[:,1] <= input_image_width) &\n",
        "             (anchor_positions[:,2] >= 0) &\n",
        "             (anchor_positions[:,3] <= input_image_height) )[0]\n",
        "\n",
        "    anchor_positions = anchor_positions[valid_anchors_idx]\n",
        "    return anchor_positions, valid_anchors_idx\n",
        "    \n",
        "# valid_anchors,valid_anchors_idx = get_valid_anchors(anchor_positions)\n",
        "# print(valid_anchors)"
      ],
      "metadata": {
        "id": "2qk8Z5Ys6FoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Getting intersection over union between 2 sets of boxes"
      ],
      "metadata": {
        "id": "eJP-ATn96Idd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Intersection over Union between all the boxes in anchor_positions and gt_boxes\n",
        "\n",
        "def get_iou_matrix(anchor_positions, gt_boxes):\n",
        "    iou_matrix = torch.zeros((len(anchor_positions), len(gt_boxes)))\n",
        "    for idx,box in enumerate(gt_boxes):\n",
        "        if isinstance(box,torch.Tensor):\n",
        "          gt = torch.cat([box]*len(anchor_positions)).view(1,-1,4)[0]\n",
        "        else:\n",
        "          gt = torch.FloatTensor([box]*len(anchor_positions))\n",
        "        max_x = torch.max(gt[:,0],anchor_positions[:,0])\n",
        "        min_x = torch.min(gt[:,1],anchor_positions[:,1])\n",
        "        max_y = torch.max(gt[:,2],anchor_positions[:,2])\n",
        "        min_y = torch.min(gt[:,3],anchor_positions[:,3])\n",
        "                \n",
        "        invalid_roi_idx = (min_x < max_x) | (min_y < max_y)\n",
        "        roi_area = (min_x - max_x)*(min_y - max_y)\n",
        "        roi_area[invalid_roi_idx] = 0\n",
        "        \n",
        "        total_area = (gt[:,1] - gt[:,0])*(gt[:,3] - gt[:,2]) + \\\n",
        "                    (anchor_positions[:,1] - anchor_positions[:,0])*(anchor_positions[:,3]-anchor_positions[:,2]) - \\\n",
        "                     roi_area\n",
        "                    \n",
        "        iou = roi_area/(total_area + 1e-6)\n",
        "        \n",
        "        iou_matrix[:,idx] = iou\n",
        "    return iou_matrix\n",
        "        "
      ],
      "metadata": {
        "id": "WyBpO92X6K5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##After IOU calculation between anchors and gt boxes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f1o0Fc-A6NRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Assign +1/-1/0 labels to anchors based on IOU\n",
        "*   Sample 128 positive and 128 negative anchors for training\n",
        "*  If less than 128 positive anchors, pad with negative\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "6nFW2A0JJpUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "    RCNN paper: \n",
        "        assign a positive label to two kinds of anchors: \n",
        "        (i) the anchor/anchors with the highest Intersection-overUnion (IoU) overlap with a ground-truth box, \n",
        "        (ii) an anchor that has an IoU overlap higher than 0.7 with\n",
        "    \n",
        "        assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 \n",
        "        Anchors that are neither positive nor negative do not contribute to the training objective.\n",
        "        \n",
        "'''\n",
        "\n",
        "def get_max_iou_data(iou_matrix):\n",
        "    gt_max_value = iou_matrix.max(axis=0)[0] #  for each gt box, this is the max iou\n",
        "    \n",
        "    #There is a possiblilty that corresponding to one gt box, there are multiple anchors with same iou (max value)\n",
        "    all_gt_max = torch.where(iou_matrix == gt_max_value)[0]\n",
        "    \n",
        "    # For each anchor box, this is the max iou with any of the gt_box\n",
        "    anchor_max_value = torch.max(iou_matrix, axis=1)[0]\n",
        "    anchor_max = torch.argmax(iou_matrix, axis=1)\n",
        "    \n",
        "    return all_gt_max, anchor_max_value,anchor_max\n",
        "\n",
        "\n",
        "\n",
        "# 1 - positive || 0 - negative || -1 - ignore\n",
        "def get_anchor_labels(anchor_positions, all_gt_max, anchor_max_value):\n",
        "    anchor_labels = torch.zeros(anchor_positions.shape[0])\n",
        "    anchor_labels.fill_(-1.0)\n",
        "\n",
        "    # for each anchor box, if iou with any of the gt_box is less than threshold, mark as 0\n",
        "    anchor_labels[anchor_max_value < 0.3] = 0\n",
        "    \n",
        "    # If corresponding to any gt_box, the anchor has max iou -> mark as 1\n",
        "    anchor_labels[all_gt_max] = 1.0\n",
        "    \n",
        "    # If for any anchor box, iou is greater than threshold for any of the gt_box, mark as 1\n",
        "    anchor_labels[anchor_max_value > 0.7] = 1.0\n",
        "    return anchor_labels"
      ],
      "metadata": {
        "id": "FeuFeHei6P4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        " RCNN paper \n",
        "    randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where\n",
        "    the sampled positive and negative anchors have a ratio of up to 1:1. If there are fewer \n",
        "    than 128 positive samples in an image, we pad the mini-batch with negative ones.\n",
        "'''\n",
        "\n",
        "def sample_anchors_for_train(anchor_labels):\n",
        "    pos_anchor_labels = torch.where(anchor_labels == 1)[0]\n",
        "    num_pos = min(num_anchors_sample/2, len(pos_anchor_labels))\n",
        "    pos_idx = np.random.choice(pos_anchor_labels,  int(num_pos), replace=False)\n",
        "\n",
        "    neg_anchor_labels = torch.where(anchor_labels == 0)[0]\n",
        "    num_neg = num_anchors_sample - num_pos\n",
        "    neg_idx = np.random.choice(neg_anchor_labels, int(num_neg), replace=False)\n",
        "    \n",
        "    anchor_labels[:] = -1\n",
        "    anchor_labels[pos_idx] = 1\n",
        "    anchor_labels[neg_idx] = 0\n",
        "    \n",
        "    return anchor_labels"
      ],
      "metadata": {
        "id": "SHomngaN6SZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##bbox regression training data calculation\n",
        "\n",
        "\n",
        "*  delta_anchor_gt - Get delta between anchor_positions and gt_boxes\n",
        "*   correct_anchor_positions - Given delta and anchor_positions, correct the anchors"
      ],
      "metadata": {
        "id": "10zVQtcl6cHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "    x,y,w,h are ctr_x, ctr_y, width and height for GT box\n",
        "    dx = (x - x_{a})/w_{a}\n",
        "    dy = (y - y_{a})/h_{a}\n",
        "    dw = log(w/ w_a)\n",
        "    dh = log(h/ h_a)\n",
        "'''\n",
        "\n",
        "# Get delta between anchor_positions and gt_boxes\n",
        "def delta_anchor_gt(anchor_positions, gt_boxes , anchor_max):\n",
        "\n",
        "    anchor_gt_map =  gt_boxes[anchor_max]\n",
        "    \n",
        "    anchor_height = anchor_positions[:,3] - anchor_positions[:,2] # y2-y1\n",
        "    anchor_width =  anchor_positions[:,1] - anchor_positions[:,0] # x2-x1\n",
        "    anchor_ctr_y = anchor_positions[:,2] + anchor_height/2 # y1 + h/2\n",
        "    anchor_ctr_x = anchor_positions[:,0] + anchor_width/2  # x1 + w/2\n",
        "    \n",
        "    gt_height = anchor_gt_map[:,3] - anchor_gt_map[:,2] # y2-y1\n",
        "    gt_width =  anchor_gt_map[:,1] - anchor_gt_map[:,0] # x2-x1\n",
        "    gt_ctr_y = anchor_gt_map[:,2] + gt_height/2 # y1 + h/2\n",
        "    gt_ctr_x = anchor_gt_map[:,0] + gt_width/2  # x1 + w/2\n",
        "    \n",
        "    dx = (gt_ctr_x - anchor_ctr_x)/anchor_width\n",
        "    dy = (gt_ctr_y - anchor_ctr_y)/anchor_height\n",
        "    dw = torch.log(gt_width/anchor_width)\n",
        "    dh = torch.log(gt_height/anchor_height)\n",
        "    \n",
        "    delta = torch.zeros_like(anchor_positions)\n",
        "    delta[:,0] = dx\n",
        "    delta[:,1] = dy\n",
        "    delta[:,2] = dw\n",
        "    delta[:,3] = dh\n",
        "   \n",
        "    return delta\n",
        "\n",
        "\n",
        "# Given delta and anchor_positions, correct the anchors\n",
        "def correct_anchor_positions(anchor_positions, delta):\n",
        "    ha = anchor_positions[:,3] - anchor_positions[:,2] # y2-y1\n",
        "    wa =  anchor_positions[:,1] - anchor_positions[:,0] # x2-x1\n",
        "    ya = anchor_positions[:,2] + ha/2 # y1 + h/2\n",
        "    xa = anchor_positions[:,0] + wa/2  # x1 + w/2\n",
        "    \n",
        "    dx = delta[:,0]\n",
        "    dy = delta[:,1]\n",
        "    dw = delta[:,2]\n",
        "    dh = delta[:,3]\n",
        "    \n",
        "    \n",
        "    x = dx*wa + xa\n",
        "    y = dy*ha + ya\n",
        "    w = torch.exp(dw)*wa\n",
        "    h = torch.exp(dh)*ha\n",
        "    \n",
        "    correct_anchor_positions = torch.zeros_like(anchor_positions)\n",
        "    \n",
        "    correct_anchor_positions[:,0] = x - w/2\n",
        "    correct_anchor_positions[:,1] = x + w/2\n",
        "    correct_anchor_positions[:,2] = y - h/2\n",
        "    correct_anchor_positions[:,3] = y + h/2\n",
        "    \n",
        "    return correct_anchor_positions"
      ],
      "metadata": {
        "id": "b25tfUea6kws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Faster RCNN region proposal network defination\n"
      ],
      "metadata": {
        "id": "znlULAZi6vZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Region Proposal Network\n",
        "\n",
        "class Faster_RCNN_rpn(nn.Module):\n",
        "    def __init__(self,extracter):\n",
        "        super().__init__()\n",
        "        self.extracter = extracter\n",
        "        self.conv1 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        #class_conv1 checks corresponding to 1 point in feature map, 18 outputs. \n",
        "        # 9 anchors * (2) || anchor has object or not\n",
        "        self.class_conv = nn.Conv2d(512, 2*len(anchor_scale)*len(anchor_ratio), 1, 1, 0)  \n",
        "        #reg_conv1 checks corresponding to 1 point in feature map, 36 outputs. \n",
        "        # 9 anchors * (4) || anchor delta wrt ground truth boxes\n",
        "        self.reg_conv = nn.Conv2d(512, 4*len(anchor_scale)*len(anchor_ratio), 1 ,1 , 0)\n",
        "\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv1.bias.data.zero_()\n",
        "        self.class_conv.weight.data.normal_(0, 0.01)\n",
        "        self.class_conv.bias.data.zero_()\n",
        "        self.reg_conv.weight.data.normal_(0, 0.01)\n",
        "        self.reg_conv.bias.data.zero_()\n",
        "\n",
        "        \n",
        "        \n",
        "    def forward(self,x):\n",
        "        # input and output features of CNN are (nSamples x nChannels x Height x Width)\n",
        "        \n",
        "        features = self.extracter(x)\n",
        "        conv1_out = F.relu(self.conv1(features))\n",
        "        class_out = self.class_conv(conv1_out)\n",
        "        reg_out = self.reg_conv(conv1_out)\n",
        "        \n",
        "        return features, class_out, reg_out"
      ],
      "metadata": {
        "id": "IqSDKQBK6wH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loss calculation - The same loss function is used for decider and RPN layer"
      ],
      "metadata": {
        "id": "c9nUUmSh61jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss for Region Proposal Netwoek\n",
        "# Same loss is used for decider network\n",
        "\n",
        "def RPN_loss(locs_preditct, class_predict, final_RPN_locs,final_RPN_class):\n",
        "    final_RPN_locs = final_RPN_locs.to(device)\n",
        "    final_RPN_class = final_RPN_class.long().to(device)\n",
        "    \n",
        "    # Cross entropy loss (check if the target is background or foreground)\n",
        "    # Only consider labels with values 1 or 0. ignore -1\n",
        "    class_loss = F.cross_entropy(class_predict, final_RPN_class, ignore_index=-1)\n",
        "\n",
        "    #smooth L1 regression loss (calculate the loss in predicted locations of foreground)\n",
        "    '''\n",
        "        Smooth L1 loss uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise\n",
        "    '''\n",
        "\n",
        "    foreground_class_idx = (final_RPN_class > 0)\n",
        "    locs_preditct  = locs_preditct[foreground_class_idx]\n",
        "    final_RPN_locs = final_RPN_locs[foreground_class_idx]\n",
        "\n",
        " \n",
        "    \n",
        "    loc_loss = F.smooth_l1_loss(locs_preditct, final_RPN_locs) / (sum(foreground_class_idx)+1e-6)\n",
        "\n",
        "    \n",
        "    rpn_loss = class_loss + rpn_loss_lambda*loc_loss\n",
        "  \n",
        "    return rpn_loss"
      ],
      "metadata": {
        "id": "dA0VgxUG63rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Non Maximum Suppression\n",
        " \n",
        "*   Custom implemented method is implemented but not used\n",
        "*   Pytorch nms method gives ~5-6 times speed up\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "hlgA_pSP65oI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply non max suppression on anchors\n",
        "\n",
        "def non_max_suppression(correct_anchor_positions, class_score, img_h, img_w, isTrain):\n",
        "    \n",
        "    if isTrain:\n",
        "        nms_pre = nms_num_train_pre\n",
        "        nms_post = nms_num_train_post\n",
        "    else :\n",
        "        nms_pre = nms_num_test_pre\n",
        "        nms_post = nms_num_test_post\n",
        "    \n",
        "    \n",
        "    # Clip the anchors to image dimensions\n",
        "    correct_anchor_positions[correct_anchor_positions[:,0] < 0,0] = 0 # x1\n",
        "    correct_anchor_positions[correct_anchor_positions[:,1] > img_w,1] = img_w # x2\n",
        "    correct_anchor_positions[correct_anchor_positions[:,2] < 0,2] = 0 # y1\n",
        "    correct_anchor_positions[correct_anchor_positions[:,3] > img_h,3] = img_h # y2\n",
        "    \n",
        "    \n",
        "    # Only keep anchors with height and width > nms_min_size\n",
        "    anchor_width = correct_anchor_positions[:,1] - correct_anchor_positions[:,0]\n",
        "    anchor_height = correct_anchor_positions[:,3] - correct_anchor_positions[:,2]\n",
        "\n",
        "\n",
        "    \n",
        "    keep_idx = (anchor_height > nms_min_size) & (anchor_width > nms_min_size)\n",
        "    correct_anchor_positions = correct_anchor_positions[keep_idx]\n",
        "    class_score = class_score[keep_idx]\n",
        "\n",
        "    # Get the index of sorted class scores in descending order and select top nms_pre idx\n",
        "    sorted_class_scores = torch.argsort(class_score, descending=True)\n",
        "    pre_nms_idx = sorted_class_scores[:nms_pre]\n",
        "    correct_anchor_positions = correct_anchor_positions[pre_nms_idx]\n",
        "    class_score = class_score[pre_nms_idx]\n",
        "    \n",
        "    # Implementation for non max suppression\n",
        "    '''\n",
        "    sorted_class_scores = torch.argsort(class_score, descending=True).to('cpu')\n",
        "    keep_anchors = []\n",
        "    \n",
        "\n",
        "    Apply NMS\n",
        "    while len(sorted_class_scores) > 1:\n",
        "        current = sorted_class_scores[0]\n",
        "        keep_anchors.append(current)\n",
        "        iou_matrix = get_iou_matrix(correct_anchor_positions[sorted_class_scores[1:]],correct_anchor_positions[current].reshape(1,-1,4)[0])\n",
        "        sorted_class_scores = sorted_class_scores[np.where(iou_matrix < nms_threshold)[0] + 1]\n",
        "    \n",
        "    if (len(sorted_class_scores) == 1):\n",
        "        keep_anchors.append(sorted_class_scores[0])\n",
        "    '''\n",
        "\n",
        "\n",
        "    '''\n",
        "      using pytorch standard nms function as it gives 5-6 times speedup\n",
        "    '''\n",
        "    change_format = torch.zeros_like(correct_anchor_positions)\n",
        "    change_format[:,0] = correct_anchor_positions[:,0]\n",
        "    change_format[:,1] = correct_anchor_positions[:,2]\n",
        "    change_format[:,2] = correct_anchor_positions[:,1]\n",
        "    change_format[:,3] = correct_anchor_positions[:,3]\n",
        "\n",
        "    keep_anchors = nms(change_format.to('cpu'), class_score.clone().detach().to('cpu'), nms_threshold)\n",
        "\n",
        "    keep_anchors = keep_anchors[:nms_post]\n",
        "    correct_anchor_positions = correct_anchor_positions[keep_anchors]\n",
        "    \n",
        "    return correct_anchor_positions\n"
      ],
      "metadata": {
        "id": "JrD1_0Yn7IMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Assign classes to output of Region Proposal Network"
      ],
      "metadata": {
        "id": "hSJ99vNZ7Rg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_classification_anchors(extracted_roi, gt_boxes, gt_labels, isTrain):\n",
        "\n",
        "    \n",
        "    # calculate iou of rois and gt boxes\n",
        "    iou_matrix = get_iou_matrix(extracted_roi, gt_boxes)\n",
        "    \n",
        "    # for each ROI, find gt with max iou and corresponding value\n",
        "    gt_roi_argmax = iou_matrix.argmax(axis=1)\n",
        "    gt_roi_max = iou_matrix.max(axis=1)[0]\n",
        "    \n",
        "    #If a particular ROI has max overlap with a gt_box, assign label of gt_box to roi\n",
        "    assign_labels = gt_labels[gt_roi_argmax]\n",
        "    \n",
        "    num_pos = pt_n_sample*pt_pos_ratio\n",
        "    pos_idx = torch.where(gt_roi_max > pt_pos_iou_threshold)[0]\n",
        "    if isTrain:\n",
        "      pos_idx = np.random.choice(pos_idx, int(min(len(pos_idx), num_pos)), replace=False)\n",
        "    \n",
        "    \n",
        "    num_neg = pt_n_sample - len(pos_idx)\n",
        "    neg_idx = torch.where(gt_roi_max < pt_neg_iou_threshold)[0]\n",
        "    if isTrain:\n",
        "      neg_idx = np.random.choice(neg_idx, int(min(len(neg_idx), num_neg)), replace=False)\n",
        "    \n",
        "    \n",
        "    keep_idx = np.append(pos_idx, neg_idx)\n",
        "    assign_labels[neg_idx] = 0\n",
        "    assign_labels = assign_labels[keep_idx]\n",
        "    extracted_roi = extracted_roi[keep_idx]\n",
        "    gt_roi_argmax = gt_roi_argmax[keep_idx]\n",
        "\n",
        "    \n",
        "    return assign_labels, extracted_roi,gt_roi_argmax, keep_idx"
      ],
      "metadata": {
        "id": "L0QO7_rL7PNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ROI Pooling Layer"
      ],
      "metadata": {
        "id": "IFO45LcB7ZEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ROI_pooling(extracted_roi, feature,ROI_pooling_layer):\n",
        "    extracted_roi = extracted_roi/16.0\n",
        "    out = []\n",
        "    for roi in extracted_roi:\n",
        "        \n",
        "        x1 = int(roi[0])\n",
        "        x2 = int(roi[1]+1)\n",
        "        y1 = int(roi[2])\n",
        "        y2 = int(roi[3]+1)\n",
        "        out.append(ROI_pooling_layer(feature[:,:,y1:y2,x1:x2]))\n",
        "    out = torch.cat(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "b9HmmXvq72mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Faster RCNN decider layer definition\n"
      ],
      "metadata": {
        "id": "mRkwpbLi7-Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Faster_RCNN_decider(nn.Module):\n",
        "    def __init__(self,classifier):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.classifier = classifier\n",
        "        self.class_lin = nn.Linear(4096, num_classes)\n",
        "        self.reg_lin = nn.Linear(4096, num_classes*4)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.classifier(x)\n",
        "        decider_class = self.class_lin(x)\n",
        "        decider_loc = self.reg_lin(x)\n",
        "        \n",
        "        return decider_class, decider_loc"
      ],
      "metadata": {
        "id": "KRIEg8WY8Arg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pipeline (train/validation) for a single image"
      ],
      "metadata": {
        "id": "0gSFWBTw8FN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_anchors_all = generate_anchors(int(input_image_height/conversion_scale), int(input_image_width/conversion_scale)).to(device)\n",
        "\n",
        "def single_image_pipeline(input_image, gt_box, label,isTrain):\n",
        "\n",
        "    input_image = input_image.to(device)\n",
        "    gt_box = torch.FloatTensor(gt_box).to(device)\n",
        "    label = torch.FloatTensor(label).to(device)\n",
        "\n",
        "    # Generate CNN features for input image\n",
        "    # Genearate region proposals predictions\n",
        "    features, class_out, reg_out = rpn(input_image)\n",
        "\n",
        "\n",
        "    locs_preditct = reg_out.permute(0,2,3,1).reshape(1,-1,4)[0]\n",
        "    class_predict = class_out.permute(0,2,3,1).reshape(1,-1,2)[0]\n",
        "    class_score = class_out.reshape(1,features.shape[2],features.shape[3],9,2)[:,:,:,:,1].reshape(1,-1)[0].detach()\n",
        "\n",
        "    \n",
        "   \n",
        "    # For training region proposal network, generate anchors on the image. \n",
        "    img_anchors_valid, img_anchors_valid_idx = get_valid_anchors(img_anchors_all.clone())\n",
        "\n",
        "    iou_anchors_gt = get_iou_matrix(img_anchors_valid, gt_box)\n",
        "    all_gt_max, anchor_max_value,anchor_max = get_max_iou_data(iou_anchors_gt)\n",
        "    \n",
        "\n",
        "    anchor_labels = get_anchor_labels(img_anchors_valid, all_gt_max, anchor_max_value)\n",
        "    anchor_labels = sample_anchors_for_train(anchor_labels)\n",
        "    \n",
        "    # TODO - check if correct. - Done\n",
        "    delta = delta_anchor_gt(img_anchors_valid, gt_box, anchor_max)\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    final_RPN_locs = torch.zeros_like(img_anchors_all)\n",
        "    final_RPN_locs[img_anchors_valid_idx] = delta\n",
        "\n",
        "    final_RPN_class = torch.zeros(img_anchors_all.shape[0])\n",
        "    final_RPN_class.fill_(-1)\n",
        "    final_RPN_class[img_anchors_valid_idx] = anchor_labels\n",
        "\n",
        "    # Loss for RPN layer\n",
        "    loss1 = RPN_loss(locs_preditct, class_predict, final_RPN_locs, final_RPN_class).to(device)\n",
        "    \n",
        "    # Based on the bbox output of rpn, correct the generated anchors\n",
        "    corrected_anchors = correct_anchor_positions(img_anchors_all.to(device), locs_preditct).detach()\n",
        "\n",
        "    \n",
        "    \n",
        "    # Apply nms on the region proposals\n",
        "    extracted_rois = non_max_suppression(corrected_anchors, class_score, input_image.shape[2],input_image.shape[3],isTrain)\n",
        "\n",
        "   \n",
        "    final_decider_class, extracted_roi_samples,gt_roi_argmax,idx = assign_classification_anchors(extracted_rois, gt_box, label, isTrain)\n",
        "\n",
        "    final_decider_locs = delta_anchor_gt(extracted_roi_samples, gt_box,gt_roi_argmax)\n",
        "   \n",
        "    # Apply ROI pooling on the extracted ROIs\n",
        "    pooled_features = ROI_pooling(extracted_roi_samples, features, ROI_pooling_layer)\n",
        "\n",
        "    decider_class, decider_loc = decider(pooled_features)\n",
        "    decider_loc = decider_loc.reshape(pooled_features.shape[0],-1,4) # 128*21*4\n",
        "    decider_loc = decider_loc[torch.arange(0,pooled_features.shape[0]), final_decider_class.long()] # 128*4\n",
        "    \n",
        "    # Loss for decider layer\n",
        "    loss2 = RPN_loss(decider_loc, decider_class, final_decider_locs, final_decider_class).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        decider_loc_no_grad = decider_loc.clone().to(device)\n",
        "\n",
        "    # Correct the ROIs based on bbox output for decider layers\n",
        "    corrected_roi = correct_anchor_positions(extracted_roi_samples,decider_loc_no_grad).detach()\n",
        "    \n",
        "    return loss1, loss2 , decider_class, corrected_roi"
      ],
      "metadata": {
        "id": "W29Bz-_W8EmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test model for input image"
      ],
      "metadata": {
        "id": "UzvN4ZLr8ME0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(input_image):\n",
        "  rpn.eval()\n",
        "  decider.eval()\n",
        "\n",
        "  input_image = input_image.to(device)\n",
        "  features, class_out, reg_out = rpn(input_image)\n",
        "  locs_preditct = reg_out.permute(0,2,3,1).reshape(1,-1,4)[0]\n",
        "  class_score = class_out.reshape(1,features.shape[2],features.shape[3],9,2)[:,:,:,:,1].reshape(1,-1)[0].detach()\n",
        "\n",
        "  corrected_anchors = correct_anchor_positions(img_anchors_all.to(device), locs_preditct).detach()\n",
        "\n",
        "    \n",
        "  extracted_rois = non_max_suppression(corrected_anchors, class_score, input_image.shape[2],input_image.shape[3],False)\n",
        "\n",
        "\n",
        "  pooled_features = ROI_pooling(extracted_rois, features, ROI_pooling_layer)\n",
        "\n",
        "  decider_class, decider_loc = decider(pooled_features)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "        decider_loc_no_grad = decider_loc.clone().to(device)\n",
        "\n",
        "  corrected_roi = correct_anchor_positions(extracted_rois,decider_loc_no_grad).detach()\n",
        "  corrected_roi[corrected_roi[:,0] < 0,0] = 0 # x1\n",
        "  corrected_roi[corrected_roi[:,1] > input_image_width,1] = input_image_width # x2\n",
        "  corrected_roi[corrected_roi[:,2] < 0,2] = 0 # y1\n",
        "  corrected_roi[corrected_roi[:,3] > input_image_height,3] = input_image_height # y2\n",
        "\n",
        "  print(corrected_roi)\n",
        "\n",
        "  decoder_conf = decider_class.softmax(dim=1).max(dim=1)[0]\n",
        "  decoder_conf = decoder_conf.detach()\n",
        "  decoder_conf = decoder_conf[decider_class.argmax(axis=1) != 0]\n",
        "  \n",
        "\n",
        "  keep_anchors = []\n",
        "  sorted_class_scores = torch.argsort(decoder_conf, descending=True)\n",
        "\n",
        "  while len(sorted_class_scores) > 1:\n",
        "    current = sorted_class_scores[0]\n",
        "    keep_anchors.append(current.item())\n",
        "    iou_matrix = get_iou_matrix(corrected_roi[sorted_class_scores[1:]],corrected_roi[current].reshape(1,-1,4)[0])\n",
        "    sorted_class_scores = sorted_class_scores[np.where(iou_matrix < 0.2)[0] + 1]\n",
        "\n",
        "  if (len(sorted_class_scores) == 1):\n",
        "    keep_anchors.append(sorted_class_scores[0].item())\n",
        "\n",
        "  for pred in decider_class.argmax(axis=1)[decider_class.argmax(axis=1) != 0][keep_anchors]:\n",
        "        print(val_to_lab[int(pred)], end=', ')\n",
        "  print('')\n",
        "  \n",
        "  for pred in decoder_conf[keep_anchors]:\n",
        "        print(pred.item(), end=', ')\n",
        "\n",
        "  visualize_tensor(input_image,extracted_rois[decider_class.argmax(axis=1) != 0][keep_anchors], [])"
      ],
      "metadata": {
        "id": "Rf90o0a08L3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Use a single image pipeline multiple times for training or batch testing\n"
      ],
      "metadata": {
        "id": "EYBULL7G8UfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define variables, models, load saved models or training steps"
      ],
      "metadata": {
        "id": "lEU-7LUvKt04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_model = ''\n",
        "\n",
        "\n",
        "\n",
        "loss1_hist = []\n",
        "loss2_hist = []\n",
        "loss_hist = []\n",
        "valid_loss1_hist = []\n",
        "valid_loss2_hist = []\n",
        "valid_loss_hist = []\n",
        "epoch_start = 0\n",
        "best_valid_score = 10000\n",
        "\n",
        "\n",
        "vgg_feature_extracter, vgg_classifier = Faster_RCNN_vgg16(num_freeze_top=10)\n",
        "rpn = Faster_RCNN_rpn(vgg_feature_extracter).to(device)\n",
        "ROI_pooling_layer = nn.AdaptiveMaxPool2d(roi_size).to(device)\n",
        "decider = Faster_RCNN_decider(vgg_classifier).to(device)\n",
        "all_params = list(list(rpn.parameters()) + list(decider.parameters()))\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(all_params, lr=0.00005)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if load_model != '':\n",
        "  print('loading model ... ')\n",
        "  checkpoint = torch.load(load_model, map_location=device)\n",
        "  rpn.load_state_dict(checkpoint['rpn_state_dict'])\n",
        "  decider.load_state_dict(checkpoint['decider_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  loss1_hist = checkpoint['loss1_hist']\n",
        "  loss2_hist = checkpoint['loss2_hist']\n",
        "  loss_hist = checkpoint['loss_hist']\n",
        "  valid_loss1_hist =checkpoint['valid_loss1_hist']\n",
        "  valid_loss2_hist = checkpoint['valid_loss2_hist']\n",
        "  valid_loss_hist = checkpoint['valid_loss_hist']\n",
        "  epoch_start = checkpoint['epoch_start']\n",
        "  best_valid_score = checkpoint['best_valid_score']\n",
        "  rpn.train()\n",
        "  decider.train()\n",
        "\n",
        "  print('model loaded ...' )\n",
        "\n",
        "\n",
        "running_count = 0\n",
        "running_net_loss = 0\n",
        "running_loss1 = 0\n",
        "running_loss2 = 0\n",
        "loss_avg_step = 20\n",
        "train_visualise_step  =150"
      ],
      "metadata": {
        "id": "9BJS7Qmu8ZfH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "f251132a38cc4b8780ffa51fd92297f5",
            "b40c85030f1443cfbd6d69b0102c71f2",
            "bcd55d44b85e44f284817eb0e420e2ac",
            "fdb03d088b4b45b09e5dc851ad544311",
            "c286e86039354e4e8391833c12712072",
            "a69721586f7e4a9b9d1ff90dfa39d814",
            "b51af0d1521d4dc791c39a0921119aea",
            "3abac43c6e374b04a5d9a22e34454f29",
            "9ec9479905ed4f56bb4374afaff16cbf",
            "0e00eccecba644ec861f4d3319584144",
            "b5260b110087412581034060352864a9"
          ]
        },
        "outputId": "3cb7586b-eca0-423b-a21a-4faf301105b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f251132a38cc4b8780ffa51fd92297f5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faster RCNN train/validation code (Supported batch size is 1)"
      ],
      "metadata": {
        "id": "dB7mXmGc8b2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start epoch - \" + str(epoch_start))\n",
        "print(\"best_valid_score - \" + str(best_valid_score))\n",
        "for epoch in range(epoch_start,epoch_start+50):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "        img, gt_box, labels = data\n",
        "\n",
        "        loss1, loss2, pred_class, pred_box = single_image_pipeline(img.to(device), gt_box, labels, True)\n",
        "        net_loss = loss1 + loss2\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        net_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss1 = loss1.detach()\n",
        "        loss2 = loss2.detach()\n",
        "        pred_class = pred_class.detach()\n",
        "        pred_box = pred_box.detach()\n",
        "        net_loss = net_loss.detach()\n",
        "        if not((math.isnan(net_loss)) or (math.isnan(loss1)) or (math.isnan(loss2))):\n",
        "            running_count += 1\n",
        "            running_net_loss += net_loss.data\n",
        "            running_loss1 += loss1.data\n",
        "            running_loss2 += loss2.data\n",
        "\n",
        "        \n",
        "        if ((i+1)%(loss_avg_step) == 0):\n",
        "            loss1_hist.append(running_loss1/(running_count + 1e-6))\n",
        "            loss2_hist.append(running_loss2/(running_count + 1e-6))\n",
        "            loss_hist.append(running_net_loss/(running_count + 1e-6))\n",
        "            print(str(epoch) + '--' + str(i) + '--- ' + str(running_net_loss/(running_count + 1e-6)))\n",
        "            running_count = 0\n",
        "            running_net_loss = 0\n",
        "            running_loss1 = 0\n",
        "            running_loss2 = 0\n",
        "\n",
        "\n",
        "        if ((i+1)%(train_visualise_step)) == 0:\n",
        "            print(\"Train Data Visualise\")\n",
        "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 5))\n",
        "            ax1.plot(loss1_hist)\n",
        "            ax1.title.set_text('Train RPN Loss')\n",
        "            ax2.plot(loss2_hist)\n",
        "            ax2.title.set_text('Train Decider Loss')\n",
        "            ax3.plot(loss_hist)\n",
        "            ax3.title.set_text('Train Net Loss')\n",
        "            visualize_tensor(img, pred_box[pred_class.argmax(axis=1) != 0], gt_box)\n",
        "            for pred in pred_class.argmax(axis=1)[pred_class.argmax(axis=1) != 0]:\n",
        "                print(val_to_lab[int(pred)], end=', ')\n",
        "            print('')\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "    '''\n",
        "    Validation Code start\n",
        "    '''\n",
        "\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=True)\n",
        "    rpn.eval()\n",
        "    decider.eval()\n",
        "    print(\"-------------------------------\")\n",
        "    print(\"Evaluating valid sets\")\n",
        "    \n",
        "    # test_model(iter(valid_loader).next()[0])\n",
        "    \n",
        "    valid_loss1 = 0\n",
        "    valid_loss2 = 0\n",
        "    valid_net_loss = 0\n",
        "    valid_run_count = 0\n",
        "    for valid_idx, valid_data in (enumerate(valid_loader,0)):\n",
        "        img, gt_box, labels = valid_data\n",
        "        loss1, loss2, pred_class, pred_box = single_image_pipeline(img.to(device), gt_box, labels, False)\n",
        "        net_loss = loss1 + loss2\n",
        "        \n",
        "        if not((math.isnan(net_loss)) or (math.isnan(loss1)) or (math.isnan(loss2))):\n",
        "            valid_loss1 += loss1.data\n",
        "            valid_loss2 += loss2.data\n",
        "            valid_net_loss += net_loss.data\n",
        "            valid_run_count += 1\n",
        "\n",
        "    \n",
        "    valid_loss1_hist.append(valid_loss1/(valid_run_count + 1e-6))\n",
        "    valid_loss2_hist.append(valid_loss2/(valid_run_count+ 1e-6))\n",
        "    valid_loss_hist.append(valid_net_loss/(valid_run_count+ 1e-6))\n",
        "    \n",
        "\n",
        "    print(\"-------------------------------\")\n",
        "    print(\"-------------------------------\")\n",
        "    print(\"Validation Data Visualise -- \")\n",
        "    print(\"-------------------------------\")\n",
        "    print(\"-------------------------------\")\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 5))\n",
        "    ax1.plot(valid_loss1_hist)\n",
        "    ax1.title.set_text('Valid RPN Loss')\n",
        "    ax2.plot(valid_loss2_hist)\n",
        "    ax2.title.set_text('Valid Decider Loss')\n",
        "    ax3.plot(valid_loss_hist)\n",
        "    ax3.title.set_text('Valid Net Loss')\n",
        "    test_model(img)\n",
        "    for pred in pred_class.argmax(axis=1)[pred_class.argmax(axis=1) != 0]:\n",
        "        print(val_to_lab[int(pred)], end=', ')\n",
        "    print('')\n",
        "    plt.show()\n",
        "        \n",
        "    \n",
        "        \n",
        "    print(valid_run_count)\n",
        "    print(\"-------------------------------\")\n",
        "    print(\"-------------------------------\")\n",
        "    rpn.train()\n",
        "    decider.train()\n",
        "\n",
        "    '''\n",
        "    Validation Code end\n",
        "    '''\n",
        "\n",
        "    PATH = 'drive/My Drive/saved_models_faster_rcnn/current.pt'\n",
        "\n",
        "    print(str(epoch)+ '--' + str(i) + ' saving model ' + PATH)\n",
        "    torch.save({\n",
        "        'rpn_state_dict': rpn.state_dict(),\n",
        "        'decider_state_dict': decider.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss1_hist':loss1_hist,\n",
        "        'loss2_hist':loss2_hist,\n",
        "        'loss_hist':loss_hist,\n",
        "        'valid_loss1_hist': valid_loss1_hist ,\n",
        "        'valid_loss2_hist':valid_loss2_hist,\n",
        "        'valid_loss_hist': valid_loss_hist,\n",
        "        'epoch_start':epoch,\n",
        "        'best_valid_score': best_valid_score\n",
        "    }, PATH)\n",
        "\n",
        "    \n",
        "    valid_net_loss = valid_net_loss/(valid_run_count+ 1e-6)\n",
        "\n",
        "    if (valid_net_loss < best_valid_score):\n",
        "        best_valid_score = valid_net_loss\n",
        "        print(\"-------------------------------\")\n",
        "        print(\"-------------------------------\")\n",
        "        print(\"Found new Best :)\")\n",
        "        print(\"-------------------------------\")\n",
        "        print(\"-------------------------------\")\n",
        "        PATH = 'drive/My Drive/saved_models_faster_rcnn/best.pt'\n",
        "\n",
        "        print(str(epoch)+ '--' + str(i) + ' saving model ' + PATH)\n",
        "        torch.save({\n",
        "            'rpn_state_dict': rpn.state_dict(),\n",
        "            'decider_state_dict': decider.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss1_hist':loss1_hist,\n",
        "            'loss2_hist':loss2_hist,\n",
        "            'loss_hist':loss_hist,\n",
        "            'valid_loss1_hist': valid_loss1_hist ,\n",
        "            'valid_loss2_hist':valid_loss2_hist,\n",
        "            'valid_loss_hist': valid_loss_hist,\n",
        "            'epoch_start':epoch,\n",
        "            'best_valid_score': best_valid_score\n",
        "        }, PATH)"
      ],
      "metadata": {
        "id": "WS1xwBwo8c_4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "outputId": "5421379c-f89d-41f4-eadd-89e0600d8cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start epoch - 0\n",
            "best_valid_score - 10000\n",
            "0--19--- tensor(1.4571, device='cuda:0')\n",
            "0--39--- tensor(0.9050, device='cuda:0')\n",
            "0--59--- tensor(0.6673, device='cuda:0')\n",
            "0--79--- tensor(0.6752, device='cuda:0')\n",
            "0--99--- tensor(0.7789, device='cuda:0')\n",
            "0--119--- tensor(0.6435, device='cuda:0')\n",
            "0--139--- tensor(0.5120, device='cuda:0')\n",
            "Train Data Visualise\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1626\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1627\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1628\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'values'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-7b4f7d005819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Data Visualise\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss1_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train RPN Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss2_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     '''\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2160x360 with 3 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABrcAAAEzCAYAAACbjVimAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYiklEQVR4nO3dYYhm910v8O+vWaNYa3vpriDZjYm4tS71QusQehG0l1bZ5MXuC7UkULQSuuC9kcu1CLkoVeKr3nIVhGjdiyVasGnsCxlwJS9qJFCakim9hiYlZYy12ShkbWvelDbm3p8v5vEynWwyZ548M3P+M58PHHjOef7M84M/M/mG757zVHcHAAAAAAAARvC6wx4AAAAAAAAAplJuAQAAAAAAMAzlFgAAAAAAAMNQbgEAAAAAADAM5RYAAAAAAADDUG4BAAAAAAAwjF3Lrar6WFU9X1VffIX3q6p+v6o2q+qJqnrH6scEABiD7AQAMJ3sBAAsY8qdWw8kOf8q79+e5OziuJTkD1/7WAAAw3ogshMAwFQPRHYCAPZo13Krux9N8vVXWXIxyZ/2lseSvKmqfnBVAwIAjER2AgCYTnYCAJaxiu/cuinJs9vOry6uAQDwcrITAMB0shMA8DInDvLDqupStm4hz+tf//qfeOtb33qQHw8A7NHnP//5f+7uU4c9x3ElOwHAWGSnwyU7AcBYXkt2WkW59VySM9vOTy+uvUx3X05yOUnW1tZ6Y2NjBR8PAOyXqvqHw57hCJKdAOCIkp32hewEAEfUa8lOq3gs4XqSX6wt70zyQnf/0wp+LgDAUSQ7AQBMJzsBAC+z651bVfWJJO9KcrKqrib5rSTflSTd/dEkV5LckWQzyTeT/PJ+DQsAMHeyEwDAdLITALCMXcut7r5rl/c7yX9d2UQAAAOTnQAAppOdAIBlrOKxhAAAAAAAAHAglFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADEO5BQAAAAAAwDCUWwAAAAAAAAxDuQUAAAAAAMAwlFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADEO5BQAAAAAAwDCUWwAAAAAAAAxDuQUAAAAAAMAwlFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADEO5BQAAAAAAwDCUWwAAAAAAAAxDuQUAAAAAAMAwlFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADEO5BQAAAAAAwDCUWwAAAAAAAAxDuQUAAAAAAMAwlFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADEO5BQAAAAAAwDCUWwAAAAAAAAxDuQUAAAAAAMAwlFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADEO5BQAAAAAAwDCUWwAAAAAAAAxDuQUAAAAAAMAwlFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADEO5BQAAAAAAwDCUWwAAAAAAAAxDuQUAAAAAAMAwlFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADGNSuVVV56vq6ararKp7r/P+zVX1SFV9oaqeqKo7Vj8qAMAYZCcAgOlkJwBgr3Ytt6rqhiT3J7k9ybkkd1XVuR3LfjPJQ9399iR3JvmDVQ8KADAC2QkAYDrZCQBYxpQ7t25Lstndz3T3i0keTHJxx5pO8v2L129M8o+rGxEAYCiyEwDAdLITALBnU8qtm5I8u+386uLadr+d5H1VdTXJlSS/er0fVFWXqmqjqjauXbu2xLgAALMnOwEATCc7AQB7Nuk7tya4K8kD3X06yR1JPl5VL/vZ3X25u9e6e+3UqVMr+mgAgOHITgAA08lOAMB3mFJuPZfkzLbz04tr292d5KEk6e7PJvmeJCdXMSAAwGBkJwCA6WQnAGDPppRbjyc5W1W3VtWN2frizvUda76a5N1JUlU/lq2Q4f5vAOA4kp0AAKaTnQCAPdu13Orul5Lck+ThJF9K8lB3P1lV91XVhcWyDyb5QFX9bZJPJHl/d/d+DQ0AMFeyEwDAdLITALCME1MWdfeVbH1h5/ZrH9r2+qkkP7na0QAAxiQ7AQBMJzsBAHs15bGEAAAAAAAAMAvKLQAAAAAAAIah3AIAAAAAAGAYyi0AAAAAAACGodwCAAAAAABgGMotAAAAAAAAhqHcAgAAAAAAYBjKLQAAAAAAAIah3AIAAAAAAGAYyi0AAAAAAACGodwCAAAAAABgGMotAAAAAAAAhqHcAgAAAAAAYBjKLQAAAAAAAIah3AIAAAAAAGAYyi0AAAAAAACGodwCAAAAAABgGMotAAAAAAAAhqHcAgAAAAAAYBjKLQAAAAAAAIah3AIAAAAAAGAYyi0AAAAAAACGodwCAAAAAABgGMotAAAAAAAAhqHcAgAAAAAAYBjKLQAAAAAAAIah3AIAAAAAAGAYyi0AAAAAAACGodwCAAAAAABgGMotAAAAAAAAhqHcAgAAAAAAYBjKLQAAAAAAAIah3AIAAAAAAGAYyi0AAAAAAACGodwCAAAAAABgGMotAAAAAAAAhqHcAgAAAAAAYBjKLQAAAAAAAIah3AIAAAAAAGAYyi0AAAAAAACGodwCAAAAAABgGMotAAAAAAAAhqHcAgAAAAAAYBjKLQAAAAAAAIah3AIAAAAAAGAYk8qtqjpfVU9X1WZV3fsKa95bVU9V1ZNV9WerHRMAYByyEwDANHITALCME7stqKobktyf5GeSXE3yeFWtd/dT29acTfI/kvxkd3+jqn5gvwYGAJgz2QkAYBq5CQBY1pQ7t25Lstndz3T3i0keTHJxx5oPJLm/u7+RJN39/GrHBAAYhuwEADCN3AQALGVKuXVTkme3nV9dXNvuLUneUlWfqarHqur8qgYEABiM7AQAMI3cBAAsZdfHEu7h55xN8q4kp5M8WlU/3t3/sn1RVV1KcilJbr755hV9NADAcGQnAIBpJuWmRHYCgONkyp1bzyU5s+389OLadleTrHf3v3b33yf5craCx3fo7svdvdbda6dOnVp2ZgCAOZOdAACmWVluSmQnADhOppRbjyc5W1W3VtWNSe5Msr5jzV9k61/QpKpOZuuW8WdWOCcAwChkJwCAaeQmAGApu5Zb3f1SknuSPJzkS0ke6u4nq+q+qrqwWPZwkq9V1VNJHkny6939tf0aGgBgrmQnAIBp5CYAYFnV3YfywWtra72xsXEonw0ATFNVn+/utcOeA9kJAEYgO82H7AQA8/dastOUxxICAAAAAADALCi3AAAAAAAAGIZyCwAAAAAAgGEotwAAAAAAABiGcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYSi3AAAAAAAAGIZyCwAAAAAAgGEotwAAAAAAABiGcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYSi3AAAAAAAAGIZyCwAAAAAAgGEotwAAAAAAABiGcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYSi3AAAAAAAAGIZyCwAAAAAAgGEotwAAAAAAABiGcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYSi3AAAAAAAAGIZyCwAAAAAAgGEotwAAAAAAABiGcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYSi3AAAAAAAAGIZyCwAAAAAAgGEotwAAAAAAABiGcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYSi3AAAAAAAAGIZyCwAAAAAAgGEotwAAAAAAABiGcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYSi3AAAAAAAAGIZyCwAAAAAAgGFMKreq6nxVPV1Vm1V176us+7mq6qpaW92IAABjkZ0AAKaTnQCAvdq13KqqG5Lcn+T2JOeS3FVV566z7g1J/luSz616SACAUchOAADTyU4AwDKm3Ll1W5LN7n6mu19M8mCSi9dZ9ztJPpzkWyucDwBgNLITAMB0shMAsGdTyq2bkjy77fzq4tr/V1XvSHKmu/9yhbMBAIxIdgIAmE52AgD2bNJ3br2aqnpdkt9N8sEJay9V1UZVbVy7du21fjQAwHBkJwCA6WQnAOB6ppRbzyU5s+389OLav3tDkrcl+Zuq+kqSdyZZv96Xe3b35e5e6+61U6dOLT81AMB8yU4AANPJTgDAnk0ptx5Pcraqbq2qG5PcmWT939/s7he6+2R339LdtyR5LMmF7t7Yl4kBAOZNdgIAmE52AgD2bNdyq7tfSnJPkoeTfCnJQ939ZFXdV1UX9ntAAICRyE4AANPJTgDAMk5MWdTdV5Jc2XHtQ6+w9l2vfSwAgHHJTgAA08lOAMBeTXksIQAAAAAAAMyCcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYSi3AAAAAAAAGIZyCwAAAAAAgGEotwAAAAAAABiGcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYSi3AAAAAAAAGIZyCwAAAAAAgGEotwAAAAAAABiGcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYSi3AAAAAAAAGIZyCwAAAAAAgGEotwAAAAAAABiGcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYSi3AAAAAAAAGIZyCwAAAAAAgGEotwAAAAAAABiGcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYSi3AAAAAAAAGIZyCwAAAAAAgGEotwAAAAAAABiGcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYSi3AAAAAAAAGIZyCwAAAAAAgGEotwAAAAAAABiGcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYSi3AAAAAAAAGIZyCwAAAAAAgGEotwAAAAAAABiGcgsAAAAAAIBhKLcAAAAAAAAYhnILAAAAAACAYUwqt6rqfFU9XVWbVXXvdd7/tap6qqqeqKpPV9UPrX5UAIAxyE4AANPITQDAMnYtt6rqhiT3J7k9ybkkd1XVuR3LvpBkrbv/Y5JPJfmfqx4UAGAEshMAwDRyEwCwrCl3bt2WZLO7n+nuF5M8mOTi9gXd/Uh3f3Nx+liS06sdEwBgGLITAMA0chMAsJQp5dZNSZ7ddn51ce2V3J3kr673RlVdqqqNqtq4du3a9CkBAMYhOwEATLOy3JTITgBwnEz6zq2pqup9SdaSfOR673f35e5e6+61U6dOrfKjAQCGIzsBAEyzW25KZCcAOE5OTFjzXJIz285PL659h6p6T5LfSPLT3f3t1YwHADAc2QkAYBq5CQBYypQ7tx5Pcraqbq2qG5PcmWR9+4KqenuSP0pyobufX/2YAADDkJ0AAKaRmwCApexabnX3S0nuSfJwki8leai7n6yq+6rqwmLZR5J8X5I/r6r/U1Xrr/DjAACONNkJAGAauQkAWNaUxxKmu68kubLj2oe2vX7PiucCABiW7AQAMI3cBAAsY8pjCQEAAAAAAGAWlFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADEO5BQAAAAAAwDCUWwAAAAAAAAxDuQUAAAAAAMAwlFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADEO5BQAAAAAAwDCUWwAAAAAAAAxDuQUAAAAAAMAwlFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADEO5BQAAAAAAwDCUWwAAAAAAAAxDuQUAAAAAAMAwlFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADEO5BQAAAAAAwDCUWwAAAAAAAAxDuQUAAAAAAMAwlFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADEO5BQAAAAAAwDCUWwAAAAAAAAxDuQUAAAAAAMAwlFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADEO5BQAAAAAAwDCUWwAAAAAAAAxDuQUAAAAAAMAwlFsAAAAAAAAMQ7kFAAAAAADAMJRbAAAAAAAADEO5BQAAAAAAwDCUWwAAAAAAAAxDuQUAAAAAAMAwlFsAAAAAAAAMQ7kFAAAAAADAMCaVW1V1vqqerqrNqrr3Ou9/d1V9cvH+56rqllUPCgAwCtkJAGA62QkA2Ktdy62quiHJ/UluT3IuyV1VdW7HsruTfKO7fyTJ7yX58KoHBQAYgewEADCd7AQALGPKnVu3Jdns7me6+8UkDya5uGPNxSR/snj9qSTvrqpa3ZgAAMOQnQAAppOdAIA9m1Ju3ZTk2W3nVxfXrrumu19K8kKSN69iQACAwchOAADTyU4AwJ6dOMgPq6pLSS4tTr9dVV88yM/nFZ1M8s+HPQT2YSbsw3zYi3n40cMe4DiTnWbJ36b5sBfzYB/mw17Mg+x0iGSnWfK3aT7sxTzYh/mwF/OwdHaaUm49l+TMtvPTi2vXW3O1qk4keWOSr+38Qd19OcnlJKmqje5eW2ZoVstezIN9mAf7MB/2Yh6qauOwZxiQ7HSE2Yf5sBfzYB/mw17Mg+y0FNnpCLMP82Ev5sE+zIe9mIfXkp2mPJbw8SRnq+rWqroxyZ1J1nesWU/yS4vXP5/kr7u7lx0KAGBgshMAwHSyEwCwZ7veudXdL1XVPUkeTnJDko9195NVdV+Sje5eT/LHST5eVZtJvp6tIAIAcOzITgAA08lOAMAyJn3nVndfSXJlx7UPbXv9rSS/sMfPvrzH9ewfezEP9mEe7MN82It5sA9LkJ2ONPswH/ZiHuzDfNiLebAPS5CdjjT7MB/2Yh7sw3zYi3lYeh/KXdwAAAAAAACMYsp3bgEAAAAAAMAs7Hu5VVXnq+rpqtqsqnuv8/53V9UnF+9/rqpu2e+ZjqMJ+/BrVfVUVT1RVZ+uqh86jDmPg932Ytu6n6uqrqq1g5zvuJiyD1X13sXvxZNV9WcHPeNxMeHv081V9UhVfWHxN+qOw5jzqKuqj1XV81X1xVd4v6rq9xf79ERVveOgZzwuZKd5kJ3mQ3aaB9lpPmSnwyc3zYfcNB+y0zzITfMhO82D3DQP+5adunvfjmx9EejfJfnhJDcm+dsk53as+S9JPrp4fWeST+7nTMfxmLgP/znJ9y5e/4p9OLy9WKx7Q5JHkzyWZO2w5z5qx8TfibNJvpDkPyzOf+Cw5z6Kx8S9uJzkVxavzyX5ymHPfRSPJD+V5B1JvvgK79+R5K+SVJJ3JvncYc98FA/ZaR6H7DSfQ3aaxyE7zeeQneZxyE3zOOSm+Ryy0zwOuWk+h+w0j0Nums+xX9lpv+/cui3JZnc/090vJnkwycUday4m+ZPF608leXdV1T7Pddzsug/d/Uh3f3Nx+liS0wc843Ex5XciSX4nyYeTfOsghztGpuzDB5Lc393fSJLufv6AZzwupuxFJ/n+xes3JvnHA5zv2OjuR5N8/VWWXEzyp73lsSRvqqofPJjpjhXZaR5kp/mQneZBdpoP2WkG5KbZkJvmQ3aaB7lpPmSneZCbZmK/stN+l1s3JXl22/nVxbXrrunul5K8kOTN+zzXcTNlH7a7O1tNKau3614sbrs8091/eZCDHTNTfifekuQtVfWZqnqsqs4f2HTHy5S9+O0k76uqq0muJPnVgxmNHfb63xKWIzvNg+w0H7LTPMhO8yE7jUFuOhhy03zITvMgN82H7DQPctM4lspOJ/ZtHIZUVe9Lspbkpw97luOoql6X5HeTvP+QR2Hr7+PZJO/K1r8oe7Sqfry7/+VQpzqe7kryQHf/r6r6T0k+XlVv6+7/d9iDAchOh0t2mhXZaT5kJ2C2ZKfDIzfNjuw0D3LTwPb7zq3nkpzZdn56ce26a6rqRLZu//vaPs913EzZh1TVe5L8RpIL3f3tA5rtuNltL96Q5G1J/qaqvpKtZ4yu+4LPlZvyO3E1yXp3/2t3/32SL2crdLBaU/bi7iQPJUl3fzbJ9yQ5eSDTsd2k/5bwmslO8yA7zYfsNA+y03zITmOQmw6G3DQfstM8yE3zITvNg9w0jqWy036XW48nOVtVt1bVjdn68s71HWvWk/zS4vXPJ/nrXnyLGCuz6z5U1duT/FG2AoZnvO6fV92L7n6hu0929y3dfUu2nkN9obs3DmfcI2vK36a/yNa/nklVnczW7eLPHOSQx8SUvfhqkncnSVX9WLaCxrUDnZJka19+sba8M8kL3f1Phz3UESQ7zYPsNB+y0zzITvMhO41BbjoYctN8yE7zIDfNh+w0D3LTOJbKTvv6WMLufqmq7knycJIbknysu5+sqvuSbHT3epI/ztbtfpvZ+lKxO/dzpuNo4j58JMn3JfnzxXerfrW7Lxza0EfUxL1gn03ch4eT/GxVPZXk/yb59e72L/xWbOJefDDJ/66q/56tL/p8v/8hXb2q+kS2gvXJxbOmfyvJdyVJd380W8+eviPJZpJvJvnlw5n0aJOd5kF2mg/ZaR5kp/mQneZBbpoHuWk+ZKd5kJvmQ3aaB7lpPvYrO5W9AgAAAAAAYBT7/VhCAAAAAAAAWBnlFgAAAAAAAMNQbgEAAAAAADAM5RYAAAAAAADDUG4BAAAAAAAwDOUWAAAAAAAAw1BuAQAAAAAAMAzlFgAAAAAAAMP4N3w435lNH8RaAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model on a single image"
      ],
      "metadata": {
        "id": "xVqE2AcA8pTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open('drive/My Drive/saved_models/bike.jpg')\n",
        "img = transform(img)\n",
        "img = img.unsqueeze(0)\n",
        "\n",
        "test_model(img)"
      ],
      "metadata": {
        "id": "6fSJLwb18nvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#References"
      ],
      "metadata": {
        "id": "2BGegOBNGnM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://paperswithcode.com/dataset/pascal-voc\n",
        "\n",
        "https://github.com/pranayKD/faster_rcnn_colab_pytorch\n",
        "\n",
        "https://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/#ITEM-1455-2"
      ],
      "metadata": {
        "id": "U3h_9OqlGL_d"
      }
    }
  ]
}